{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "client_bq = bigquery.Client.from_service_account_json(\"./credentials.json\", project='charged-dialect-824')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data from BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_bq_data(_sql):\n",
    "    _df = client_bq.query(_sql).to_dataframe()\n",
    "    return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in the dataset:  37567\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>articleId</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>productType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3393892867263864215</td>\n",
       "      <td>Gone Is Gone - Echolocation</td>\n",
       "      <td>Gone Is Gone - Echolocation</td>\n",
       "      <td>cd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2831118468819601923</td>\n",
       "      <td>Ekseption-The Lost Last Concert Tapes...</td>\n",
       "      <td>The Lost Last Concert Tapes (Box-Set)</td>\n",
       "      <td>cd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6835807414137785977</td>\n",
       "      <td>PORTISHEAD CD-Sammlung 3 CDs #16</td>\n",
       "      <td>Kollektion für Fans der Trip-Hop-Pioniere aus ...</td>\n",
       "      <td>cd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-4912458353746230865</td>\n",
       "      <td>Primeon CD-R Rohlinge (bunt)</td>\n",
       "      <td>39 Stück auf der Spindel</td>\n",
       "      <td>cd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1496864991301777371</td>\n",
       "      <td>Adonia - Zmittst im Füür</td>\n",
       "      <td>Markus Hottiger / Markus Heusser</td>\n",
       "      <td>cd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             articleId                                     title  \\\n",
       "0 -3393892867263864215               Gone Is Gone - Echolocation   \n",
       "1 -2831118468819601923  Ekseption-The Lost Last Concert Tapes...   \n",
       "2  6835807414137785977          PORTISHEAD CD-Sammlung 3 CDs #16   \n",
       "3 -4912458353746230865              Primeon CD-R Rohlinge (bunt)   \n",
       "4  1496864991301777371                  Adonia - Zmittst im Füür   \n",
       "\n",
       "                                            subtitle productType  \n",
       "0                        Gone Is Gone - Echolocation          cd  \n",
       "1              The Lost Last Concert Tapes (Box-Set)          cd  \n",
       "2  Kollektion für Fans der Trip-Hop-Pioniere aus ...          cd  \n",
       "3                           39 Stück auf der Spindel          cd  \n",
       "4                   Markus Hottiger / Markus Heusser          cd  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "SELECT *\n",
    "FROM RicardoInterview.product_detection_training_data\n",
    "\"\"\"\n",
    "\n",
    "df_load = load_bq_data(sql)\n",
    "print(\"Number of entries in the dataset: \", len(df_load))\n",
    "df_load.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To be implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\marco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import joblib\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Firstly we are going to look at the number of possible class and their distribution\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    383.000000\n",
       "mean      98.086162\n",
       "std        5.492059\n",
       "min       48.000000\n",
       "25%       99.000000\n",
       "50%       99.000000\n",
       "75%       99.000000\n",
       "max       99.000000\n",
       "Name: productType, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Firstly we are going to look at the number of possible class and their distribution\")\n",
    "df_count = df_load[\"productType\"].value_counts()\n",
    "# Calculate summary statistics\n",
    "df_count.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are going to look at the length of the text we have to train our models\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>articleId</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>productType</th>\n",
       "      <th>length_title</th>\n",
       "      <th>length_subtitle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27300</th>\n",
       "      <td>-5186182111564884197</td>\n",
       "      <td>Autotransporter / Universaltransporter TPV / B...</td>\n",
       "      <td></td>\n",
       "      <td>trailer</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8533</th>\n",
       "      <td>-6183040234950825017</td>\n",
       "      <td>Speditionsanhänger mit Hebebühne D'Hollandia m...</td>\n",
       "      <td>schöner Profi Anhänger</td>\n",
       "      <td>trailer</td>\n",
       "      <td>60</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1211</th>\n",
       "      <td>4782662448248258064</td>\n",
       "      <td>5.5 KR-Seekreuzer Yachtwerft Stäheli CH-8595 A...</td>\n",
       "      <td>Einmalige Gelegenheit für Liebhaber klassische...</td>\n",
       "      <td>boat</td>\n",
       "      <td>60</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8561</th>\n",
       "      <td>-6437718805549010031</td>\n",
       "      <td>Kipper, 3-Seitenkipper, 3000kg, Pongratz 3-SKS...</td>\n",
       "      <td>Heckkipper Alukipper M.B. Stahlkipper Wenk MB ...</td>\n",
       "      <td>trailer</td>\n",
       "      <td>60</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2175</th>\n",
       "      <td>-2063386134032828754</td>\n",
       "      <td>Anhänger für Schneemobil  Anhänger   Anhänger ...</td>\n",
       "      <td>Garag Jann Graf Trimmis</td>\n",
       "      <td>quad</td>\n",
       "      <td>60</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 articleId                                              title  \\\n",
       "27300 -5186182111564884197  Autotransporter / Universaltransporter TPV / B...   \n",
       "8533  -6183040234950825017  Speditionsanhänger mit Hebebühne D'Hollandia m...   \n",
       "1211   4782662448248258064  5.5 KR-Seekreuzer Yachtwerft Stäheli CH-8595 A...   \n",
       "8561  -6437718805549010031  Kipper, 3-Seitenkipper, 3000kg, Pongratz 3-SKS...   \n",
       "2175  -2063386134032828754  Anhänger für Schneemobil  Anhänger   Anhänger ...   \n",
       "\n",
       "                                                subtitle productType  \\\n",
       "27300                                                        trailer   \n",
       "8533                              schöner Profi Anhänger     trailer   \n",
       "1211   Einmalige Gelegenheit für Liebhaber klassische...        boat   \n",
       "8561   Heckkipper Alukipper M.B. Stahlkipper Wenk MB ...     trailer   \n",
       "2175                             Garag Jann Graf Trimmis        quad   \n",
       "\n",
       "       length_title  length_subtitle  \n",
       "27300            60                0  \n",
       "8533             60               22  \n",
       "1211             60               62  \n",
       "8561             60               75  \n",
       "2175             60               23  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"We are going to look at the length of the text we have to train our models\")\n",
    "df_load[\"length_title\"]= df_load[\"title\"].apply(lambda x: len(x))\n",
    "df_load[\"length_subtitle\"] = df_load[\"subtitle\"].apply(lambda x: len(x))\n",
    "df_load.sort_values(by=\"length_title\", ascending = False).head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have very short description with few information in the title and subtitle, we are going to look at different method to encode our textual data. First we will do some small preprocessing on the data and then try to encode it to vector keeping some relations on the words.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: CountVectorizer\n",
    "First, since the titles and subtitles seem to have little semantic relations, only key words, brands in multiple languages we will use CountVectorizer that onvert text into a numerical vector representation based on word frequencies. We will use the results to then train a Random Forest Classifier and look at its accuracy on test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we are going to look into CountVectorizer\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_load[['title', 'subtitle']], df_load['productType'], test_size=0.3, stratify = df_load['productType'], random_state=42)\n",
    "\n",
    "X_train_combined = X_train['title'] + \" \" + X_train['subtitle']\n",
    "X_test_combined = X_test['title'] + \" \" + X_test['subtitle']\n",
    "\n",
    "# Convert the preprocessed text into count-based vectors using CountVectorizer\n",
    "count_vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "X_train_count = count_vectorizer.fit_transform(X_train_combined)\n",
    "X_test_count = count_vectorizer.transform(X_test_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6657794339455239\n"
     ]
    }
   ],
   "source": [
    "# Train a classifier model on the encoded features\n",
    "classifier = RandomForestClassifier()\n",
    "classifier.fit(X_train_count, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_countVect = classifier.predict(X_test_count)\n",
    "\n",
    "# Evaluate the classifier performance\n",
    "print(accuracy_score(y_test, y_pred_countVect))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Word2Vec\n",
    "\n",
    "To try and improve on the previous score we will look at another method rather then using a CountVectorizer. For this we would like to create a Word2Vec model, which creates word embeddings capturing semantic relationships in continuous vector space. From this word embedding we will get some numerical features using TF-IDF: that uses the importance of a word in the text relative to its importance in all text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the text data (lowercase conversion and stopwords removal)\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "german_stopwords_set = set(stopwords.words('german'))\n",
    "stopwords_set.update(german_stopwords_set)  # Add German stopwords to the set\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove stopwords\n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords_set])\n",
    "    return text\n",
    "\n",
    "X_train_processed = X_train_combined.apply(preprocess_text)\n",
    "X_test_processed = X_test_combined.apply(preprocess_text)\n",
    "\n",
    "#Create a word2Vec model with the tokenized text\n",
    "tokenized_text = [text.split() for text in X_train_processed]\n",
    "word2vec_model = Word2Vec(tokenized_text, vector_size=100, window=5, min_count=1)\n",
    "\n",
    "# Encode the text features using the trained Word2Vec model\n",
    "def encode_text(text):\n",
    "    vector = []\n",
    "    for word in text.split():\n",
    "        if word in word2vec_model.wv:\n",
    "            vector.append(word)\n",
    "    return \" \".join(vector)\n",
    "\n",
    "X_train_word2vec = X_train_processed.apply(encode_text)\n",
    "X_test_word2vec = X_test_processed.apply(encode_text)\n",
    "\n",
    "# Convert the encoded features into numerical vectors using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf_word2vec = tfidf_vectorizer.fit_transform(X_train_word2vec)\n",
    "X_test_tfidf_word2vec = tfidf_vectorizer.transform(X_test_word2vec)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6548664714754681\n"
     ]
    }
   ],
   "source": [
    "# Train a classifier model on the encoded features\n",
    "classifier.fit(X_train_tfidf_word2vec, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_word2vec = classifier.predict(X_test_tfidf_word2vec)\n",
    "\n",
    "# Evaluate the classifier performance\n",
    "print(accuracy_score(y_test, y_pred_word2vec))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Using Distilled Bert Multi-lingual model\n",
    "\n",
    "It seems that for this specific problem semantic of the world do not improve our model, this could come from the fact that the description are short and multi-lingual. We thus could improve on this solution with more pre-processing and looking at putting semantic of different language together with model from spacy for example in a future tasks.\n",
    "\n",
    "\n",
    "If more computational power and longer training was needed we could use multi-lingual pretrained model such as distilled-bert-mulitlingual to encode the data and train the classifier. For this case code would look as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "model_name ='distilbert-base-multilingual-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# Define batch size for processing in smaller batches\n",
    "batch_size = 32\n",
    "\n",
    "# Initialize lists to store encoded embeddings\n",
    "X_train_embeddings = []\n",
    "X_test_embeddings = []\n",
    "\n",
    "# Process data in smaller batches\n",
    "for i in range(0, len(X_train_combined), batch_size):\n",
    "    batch_texts = X_train_combined[i : i + batch_size]\n",
    "\n",
    "    # Tokenize the text using BERT tokenizer\n",
    "    batch_encoded = []\n",
    "    for text in batch_texts:\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "        batch_encoded.append(tokens)\n",
    "\n",
    "    # Pad the sequences to a fixed length\n",
    "    max_length = max(len(tokens) for tokens in batch_encoded)\n",
    "    batch_padded = [tokens + [0] * (max_length - len(tokens)) for tokens in batch_encoded]\n",
    "\n",
    "    # Convert the padded sequences to tensors\n",
    "    batch_tensors = torch.tensor(batch_padded)\n",
    "\n",
    "    # Use BERT model to encode the text\n",
    "    with torch.no_grad():\n",
    "        batch_embeddings = model(batch_tensors)[0][:, 0, :]\n",
    "        X_train_embeddings.extend(batch_embeddings.tolist())\n",
    "\n",
    "# Process test data in smaller batches\n",
    "for i in range(0, len(X_test_combined), batch_size):\n",
    "    batch_texts = X_test_combined[i : i + batch_size]\n",
    "\n",
    "    # Tokenize and encode the text using BERT tokenizer\n",
    "    batch_encoded = []\n",
    "    for text in batch_texts:\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "        batch_encoded.append(tokens)\n",
    "\n",
    "    # Pad the sequences to a fixed length\n",
    "    max_length = max(len(tokens) for tokens in batch_encoded)\n",
    "    batch_padded = [tokens + [0] * (max_length - len(tokens)) for tokens in batch_encoded]\n",
    "\n",
    "    # Convert the padded sequences to tensors\n",
    "    batch_tensors = torch.tensor(batch_padded)\n",
    "\n",
    "    # Use BERT model to encode the text\n",
    "    with torch.no_grad():\n",
    "        batch_embeddings = model(batch_tensors)[0][:, 0, :]\n",
    "        X_test_embeddings.extend(batch_embeddings.tolist())\n",
    "\n",
    "# Convert the encoded embeddings to tensors\n",
    "X_train_tensors = torch.tensor(X_train_embeddings)\n",
    "X_test_tensors = torch.tensor(X_test_embeddings)\n",
    "\n",
    "# We can then use the same classifier for the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the classification model\n",
    "with open('models/classifier_model_countVectorizer.joblib', 'wb') as file:\n",
    "    joblib.dump(classifier, file, compress=('gzip', 3))\n",
    "\n",
    "# Save the CountVectorizer model\n",
    "with open(\"models/count_vectorizer.joblib\", \"wb\") as file:\n",
    "    joblib.dump(count_vectorizer, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
